{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from dotenv_vault import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = os.getenv('TOKENIZE_NOTES')\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Drop the 'filename' column as it is not needed for training\n",
    "df = df.drop(columns=['filename'])\n",
    "\n",
    "# Split data into features and labels\n",
    "X = df.drop(columns=['word_count', 'content_length'])\n",
    "y = df['word_count']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Normalize the target (if needed)\n",
    "y_train_mean = y_train.mean()\n",
    "y_train_std = y_train.std()\n",
    "y_train_scaled = (y_train - y_train_mean) / y_train_std\n",
    "y_test_scaled = (y_test - y_train_mean) / y_train_std\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_scaled.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_scaled.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotesDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = NotesDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = NotesDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EnhancedNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(EnhancedNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, 1)  # For regression, output a single value\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "# Model parameters\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "hidden_dim = 750\n",
    "\n",
    "# Initialize the model\n",
    "# model = DeeperNN(input_dim, hidden_dim)\n",
    "model = EnhancedNN(input_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 0.2903764908275518\n",
      "Epoch 2/300, Loss: 0.022266520089224764\n",
      "Epoch 3/300, Loss: 0.005258063270287637\n",
      "Epoch 4/300, Loss: 0.00048674829036156694\n",
      "Epoch 5/300, Loss: 0.00024298603459197023\n",
      "Epoch 6/300, Loss: 0.00015591201324576488\n",
      "Epoch 7/300, Loss: 8.26770594693172e-05\n",
      "Epoch 8/300, Loss: 6.814811313473144e-05\n",
      "Epoch 9/300, Loss: 8.32345378129455e-05\n",
      "Epoch 10/300, Loss: 7.883314906633275e-05\n",
      "Epoch 11/300, Loss: 6.973866319233486e-05\n",
      "Epoch 12/300, Loss: 7.031092634765388e-05\n",
      "Epoch 13/300, Loss: 9.569844400845652e-05\n",
      "Epoch 14/300, Loss: 5.253113068934717e-05\n",
      "Epoch 15/300, Loss: 0.00010377406901122748\n",
      "Epoch 16/300, Loss: 6.651196932388732e-05\n",
      "Epoch 17/300, Loss: 7.295655814232305e-05\n",
      "Epoch 18/300, Loss: 4.7063395490379446e-05\n",
      "Epoch 19/300, Loss: 3.93966793724662e-05\n",
      "Epoch 20/300, Loss: 2.1504269338229758e-05\n",
      "Epoch 21/300, Loss: 3.5314220021494914e-05\n",
      "Epoch 22/300, Loss: 3.537893271162542e-05\n",
      "Epoch 23/300, Loss: 5.4499739402863746e-05\n",
      "Epoch 24/300, Loss: 6.812281507288724e-05\n",
      "Epoch 25/300, Loss: 0.0008787331744554793\n",
      "Epoch 26/300, Loss: 0.0005593707764163762\n",
      "Epoch 27/300, Loss: 0.00011272953174499746\n",
      "Epoch 28/300, Loss: 5.8996489523104556e-05\n",
      "Epoch 29/300, Loss: 7.975955953186617e-05\n",
      "Epoch 30/300, Loss: 0.00011281513476253742\n",
      "Epoch 31/300, Loss: 4.4698836584182955e-05\n",
      "Epoch 32/300, Loss: 3.079948929122398e-05\n",
      "Epoch 33/300, Loss: 0.00018139543074443951\n",
      "Epoch 34/300, Loss: 0.0011908985213123838\n",
      "Epoch 35/300, Loss: 0.0023652116575057767\n",
      "Epoch 36/300, Loss: 0.0021899440078318308\n",
      "Epoch 37/300, Loss: 0.0007273053375492486\n",
      "Epoch 38/300, Loss: 0.0014137451130648657\n",
      "Epoch 39/300, Loss: 0.0012490793330806283\n",
      "Epoch 40/300, Loss: 0.00167319069585478\n",
      "Epoch 41/300, Loss: 0.0023967220790118995\n",
      "Epoch 42/300, Loss: 0.002583866987582664\n",
      "Epoch 43/300, Loss: 0.0006072524773499984\n",
      "Epoch 44/300, Loss: 0.0005756708094275765\n",
      "Epoch 45/300, Loss: 0.0007181885014831909\n",
      "Epoch 46/300, Loss: 0.0005004577970602872\n",
      "Epoch 47/300, Loss: 0.0001301178685935156\n",
      "Epoch 48/300, Loss: 0.0001893123978669265\n",
      "Epoch 49/300, Loss: 9.313342376883252e-05\n",
      "Epoch 50/300, Loss: 6.103733740976742e-05\n",
      "Epoch 51/300, Loss: 5.8102220284701233e-05\n",
      "Epoch 52/300, Loss: 3.538291243905022e-05\n",
      "Epoch 53/300, Loss: 5.107772493691922e-05\n",
      "Epoch 54/300, Loss: 7.71718628369088e-05\n",
      "Epoch 55/300, Loss: 0.0003122144033975354\n",
      "Epoch 56/300, Loss: 0.0007406804463160274\n",
      "Epoch 57/300, Loss: 0.0004856226163323491\n",
      "Epoch 58/300, Loss: 0.00039349063532774\n",
      "Epoch 59/300, Loss: 0.00027108189413794256\n",
      "Epoch 60/300, Loss: 8.689711725501663e-05\n",
      "Epoch 61/300, Loss: 0.0004709033202480465\n",
      "Epoch 62/300, Loss: 0.00024451843783505244\n",
      "Epoch 63/300, Loss: 5.802033408479857e-05\n",
      "Epoch 64/300, Loss: 4.358902870798805e-05\n",
      "Epoch 65/300, Loss: 0.00020353608603105195\n",
      "Epoch 66/300, Loss: 0.00011593035714883484\n",
      "Epoch 67/300, Loss: 9.720802725161392e-05\n",
      "Epoch 68/300, Loss: 5.9910570999626976e-05\n",
      "Epoch 69/300, Loss: 6.900374930745257e-05\n",
      "Epoch 70/300, Loss: 0.00017553107366103702\n",
      "Epoch 71/300, Loss: 3.668165173789212e-05\n",
      "Epoch 72/300, Loss: 8.907769250455251e-05\n",
      "Epoch 73/300, Loss: 6.673422033511962e-05\n",
      "Epoch 74/300, Loss: 0.00020840658411495832\n",
      "Epoch 75/300, Loss: 0.00532140533158853\n",
      "Epoch 76/300, Loss: 0.005050722004326465\n",
      "Epoch 77/300, Loss: 0.01839837131831844\n",
      "Epoch 78/300, Loss: 0.00678811587510465\n",
      "Epoch 79/300, Loss: 0.0007687303138817171\n",
      "Epoch 80/300, Loss: 0.0007219134833432786\n",
      "Epoch 81/300, Loss: 0.00027587693526392716\n",
      "Epoch 82/300, Loss: 0.00025983777246246494\n",
      "Epoch 83/300, Loss: 0.0013674891030716122\n",
      "Epoch 84/300, Loss: 0.00021301494360302873\n",
      "Epoch 85/300, Loss: 0.00013729795566513005\n",
      "Epoch 86/300, Loss: 0.0005125014635041225\n",
      "Epoch 87/300, Loss: 0.0003804814252457857\n",
      "Epoch 88/300, Loss: 0.00011542282328491638\n",
      "Epoch 89/300, Loss: 3.3292168597510775e-05\n",
      "Epoch 90/300, Loss: 7.752202751170874e-05\n",
      "Epoch 91/300, Loss: 8.677500959059241e-05\n",
      "Epoch 92/300, Loss: 2.1367405894891005e-05\n",
      "Epoch 93/300, Loss: 1.7063462415104657e-05\n",
      "Epoch 94/300, Loss: 3.9495705412886504e-05\n",
      "Epoch 95/300, Loss: 2.7946423259843295e-05\n",
      "Epoch 96/300, Loss: 2.416534893630964e-05\n",
      "Epoch 97/300, Loss: 2.2870822515654398e-05\n",
      "Epoch 98/300, Loss: 3.473003287904488e-05\n",
      "Epoch 99/300, Loss: 1.2616959293753002e-05\n",
      "Epoch 100/300, Loss: 4.440833670646258e-05\n",
      "Epoch 101/300, Loss: 2.700677484462607e-05\n",
      "Epoch 102/300, Loss: 5.5117936324821036e-05\n",
      "Epoch 103/300, Loss: 5.270430852090756e-05\n",
      "Epoch 104/300, Loss: 0.0001248341557009936\n",
      "Epoch 105/300, Loss: 0.0006374940615531063\n",
      "Epoch 106/300, Loss: 0.00542698060158536\n",
      "Epoch 107/300, Loss: 0.004160892702011073\n",
      "Epoch 108/300, Loss: 0.02946176515845429\n",
      "Epoch 109/300, Loss: 0.0019602186267117135\n",
      "Epoch 110/300, Loss: 0.0002539836741083204\n",
      "Epoch 111/300, Loss: 0.0002689608461303961\n",
      "Epoch 112/300, Loss: 5.175908588745714e-05\n",
      "Epoch 113/300, Loss: 3.240922643798465e-05\n",
      "Epoch 114/300, Loss: 0.00010440058890468975\n",
      "Epoch 115/300, Loss: 0.0004129109545494047\n",
      "Epoch 116/300, Loss: 0.0017342175726300837\n",
      "Epoch 117/300, Loss: 0.0023491168039024494\n",
      "Epoch 118/300, Loss: 0.0021815978846126726\n",
      "Epoch 119/300, Loss: 0.0023649421439969095\n",
      "Epoch 120/300, Loss: 0.0020596196669238885\n",
      "Epoch 121/300, Loss: 0.0003730044757706932\n",
      "Epoch 122/300, Loss: 0.00017791723291195108\n",
      "Epoch 123/300, Loss: 0.00030831781185425874\n",
      "Epoch 124/300, Loss: 0.00042894063903986535\n",
      "Epoch 125/300, Loss: 0.0002833510306788268\n",
      "Epoch 126/300, Loss: 7.886508281348812e-05\n",
      "Epoch 127/300, Loss: 3.750546110088755e-05\n",
      "Epoch 128/300, Loss: 9.020679517322463e-05\n",
      "Epoch 129/300, Loss: 3.1891652569308794e-05\n",
      "Epoch 130/300, Loss: 2.7620802962988627e-05\n",
      "Epoch 131/300, Loss: 3.143093356596326e-05\n",
      "Epoch 132/300, Loss: 3.843407516438004e-05\n",
      "Epoch 133/300, Loss: 0.0001625163524151599\n",
      "Epoch 134/300, Loss: 9.777151375314325e-05\n",
      "Epoch 135/300, Loss: 5.5417886253388515e-05\n",
      "Epoch 136/300, Loss: 0.0004502446786797296\n",
      "Epoch 137/300, Loss: 0.0005150448794988973\n",
      "Epoch 138/300, Loss: 0.00014436252356039118\n",
      "Epoch 139/300, Loss: 9.701552265764086e-05\n",
      "Epoch 140/300, Loss: 4.884299127040418e-05\n",
      "Epoch 141/300, Loss: 9.394114147198707e-05\n",
      "Epoch 142/300, Loss: 2.6187421293630714e-05\n",
      "Epoch 143/300, Loss: 5.379839725911121e-05\n",
      "Epoch 144/300, Loss: 2.5549947245532455e-05\n",
      "Epoch 145/300, Loss: 6.348329558130372e-05\n",
      "Epoch 146/300, Loss: 4.160432640494611e-05\n",
      "Epoch 147/300, Loss: 7.501960233843132e-05\n",
      "Epoch 148/300, Loss: 7.369549522787853e-05\n",
      "Epoch 149/300, Loss: 4.173530485137402e-05\n",
      "Epoch 150/300, Loss: 1.6589480860841976e-05\n",
      "Epoch 151/300, Loss: 3.317229616366029e-05\n",
      "Epoch 152/300, Loss: 1.1092417959221884e-05\n",
      "Epoch 153/300, Loss: 1.043044997722494e-05\n",
      "Epoch 154/300, Loss: 9.097338701586092e-06\n",
      "Epoch 155/300, Loss: 9.546277506220385e-06\n",
      "Epoch 156/300, Loss: 7.102042842102731e-06\n",
      "Epoch 157/300, Loss: 8.884369812652662e-06\n",
      "Epoch 158/300, Loss: 6.381952972919623e-06\n",
      "Epoch 159/300, Loss: 1.3938194867753401e-05\n",
      "Epoch 160/300, Loss: 2.034227414782208e-05\n",
      "Epoch 161/300, Loss: 8.323327149003932e-06\n",
      "Epoch 162/300, Loss: 1.7977931864789442e-05\n",
      "Epoch 163/300, Loss: 1.5480727424083423e-05\n",
      "Epoch 164/300, Loss: 9.667116285614809e-05\n",
      "Epoch 165/300, Loss: 4.8623169571502136e-05\n",
      "Epoch 166/300, Loss: 0.00019585371183582443\n",
      "Epoch 167/300, Loss: 0.00019239799165629128\n",
      "Epoch 168/300, Loss: 5.116828873197894e-05\n",
      "Epoch 169/300, Loss: 1.9586506428631285e-05\n",
      "Epoch 170/300, Loss: 4.198055554495209e-05\n",
      "Epoch 171/300, Loss: 0.00032161589157459803\n",
      "Epoch 172/300, Loss: 5.80200762370671e-05\n",
      "Epoch 173/300, Loss: 0.0002575878059504152\n",
      "Epoch 174/300, Loss: 0.0004297656702371904\n",
      "Epoch 175/300, Loss: 7.65742800551475e-05\n",
      "Epoch 176/300, Loss: 2.524694019113763e-05\n",
      "Epoch 177/300, Loss: 2.8564013901862046e-05\n",
      "Epoch 178/300, Loss: 5.815043876485728e-05\n",
      "Epoch 179/300, Loss: 5.2525763521771296e-05\n",
      "Epoch 180/300, Loss: 0.0001396754191594147\n",
      "Epoch 181/300, Loss: 6.826411002242893e-05\n",
      "Epoch 182/300, Loss: 0.00024440374523680334\n",
      "Epoch 183/300, Loss: 0.00024359958140513825\n",
      "Epoch 184/300, Loss: 4.0961973178055864e-05\n",
      "Epoch 185/300, Loss: 1.5007289074506652e-05\n",
      "Epoch 186/300, Loss: 5.996774373634732e-06\n",
      "Epoch 187/300, Loss: 5.688035265157061e-06\n",
      "Epoch 188/300, Loss: 5.971539947250006e-06\n",
      "Epoch 189/300, Loss: 2.0468187795143566e-05\n",
      "Epoch 190/300, Loss: 2.329695661273945e-05\n",
      "Epoch 191/300, Loss: 7.822700121993733e-05\n",
      "Epoch 192/300, Loss: 0.0023538204129516472\n",
      "Epoch 193/300, Loss: 0.006279635582790146\n",
      "Epoch 194/300, Loss: 0.013042914652798668\n",
      "Epoch 195/300, Loss: 0.006287369366993134\n",
      "Epoch 196/300, Loss: 0.012733489753947268\n",
      "Epoch 197/300, Loss: 0.007255086174968471\n",
      "Epoch 198/300, Loss: 0.01757948721091165\n",
      "Epoch 199/300, Loss: 0.013517858739233864\n",
      "Epoch 200/300, Loss: 0.005552777555605739\n",
      "Epoch 201/300, Loss: 0.0020664274427776705\n",
      "Epoch 202/300, Loss: 0.001576173463035957\n",
      "Epoch 203/300, Loss: 0.002104670271735717\n",
      "Epoch 204/300, Loss: 0.0020863552618290045\n",
      "Epoch 205/300, Loss: 0.00019870360026623914\n",
      "Epoch 206/300, Loss: 9.830077152927468e-05\n",
      "Epoch 207/300, Loss: 0.00011028590561660454\n",
      "Epoch 208/300, Loss: 8.465934531374699e-05\n",
      "Epoch 209/300, Loss: 7.775356968259554e-05\n",
      "Epoch 210/300, Loss: 3.672966354584835e-05\n",
      "Epoch 211/300, Loss: 3.793814236312181e-05\n",
      "Epoch 212/300, Loss: 2.9389115296321745e-05\n",
      "Epoch 213/300, Loss: 9.874027486938231e-05\n",
      "Epoch 214/300, Loss: 7.028458218739711e-05\n",
      "Epoch 215/300, Loss: 0.0001134355844579946\n",
      "Epoch 216/300, Loss: 0.0003891497230453847\n",
      "Epoch 217/300, Loss: 0.0004012308625138566\n",
      "Epoch 218/300, Loss: 3.49331731459451e-05\n",
      "Epoch 219/300, Loss: 8.695315195806965e-05\n",
      "Epoch 220/300, Loss: 5.185967952277344e-05\n",
      "Epoch 221/300, Loss: 1.5219805971499131e-05\n",
      "Epoch 222/300, Loss: 1.9468998743832344e-05\n",
      "Epoch 223/300, Loss: 1.5814723440144007e-05\n",
      "Epoch 224/300, Loss: 1.5912585841928567e-05\n",
      "Epoch 225/300, Loss: 1.8576709027752857e-05\n",
      "Epoch 226/300, Loss: 1.5216339997885753e-05\n",
      "Epoch 227/300, Loss: 3.7938607764126096e-05\n",
      "Epoch 228/300, Loss: 2.1289718802856777e-05\n",
      "Epoch 229/300, Loss: 2.755385610959437e-05\n",
      "Epoch 230/300, Loss: 2.084040128717166e-05\n",
      "Epoch 231/300, Loss: 2.425503856335653e-05\n",
      "Epoch 232/300, Loss: 1.7383050999192644e-05\n",
      "Epoch 233/300, Loss: 2.3421899255170404e-05\n",
      "Epoch 234/300, Loss: 1.2978324288849907e-05\n",
      "Epoch 235/300, Loss: 2.0359593402250348e-05\n",
      "Epoch 236/300, Loss: 9.497150553048222e-06\n",
      "Epoch 237/300, Loss: 1.3285319529119844e-05\n",
      "Epoch 238/300, Loss: 9.515429393571284e-06\n",
      "Epoch 239/300, Loss: 9.313020828621551e-06\n",
      "Epoch 240/300, Loss: 1.2119223651109822e-05\n",
      "Epoch 241/300, Loss: 1.221043431829266e-05\n",
      "Epoch 242/300, Loss: 9.268388122575887e-06\n",
      "Epoch 243/300, Loss: 1.634321033873819e-05\n",
      "Epoch 244/300, Loss: 1.7249499595245172e-05\n",
      "Epoch 245/300, Loss: 3.716314345662173e-05\n",
      "Epoch 246/300, Loss: 2.7710372870315997e-05\n",
      "Epoch 247/300, Loss: 0.00011292460299584586\n",
      "Epoch 248/300, Loss: 0.00012914102435508155\n",
      "Epoch 249/300, Loss: 0.0001650498429416745\n",
      "Epoch 250/300, Loss: 0.0001780623723980649\n",
      "Epoch 251/300, Loss: 3.576765166862491e-05\n",
      "Epoch 252/300, Loss: 1.803049252956905e-05\n",
      "Epoch 253/300, Loss: 2.2430751117571834e-05\n",
      "Epoch 254/300, Loss: 1.1868130045693187e-05\n",
      "Epoch 255/300, Loss: 1.1644219221962934e-05\n",
      "Epoch 256/300, Loss: 8.342240970418253e-05\n",
      "Epoch 257/300, Loss: 7.017012925446484e-05\n",
      "Epoch 258/300, Loss: 1.5437008049670303e-05\n",
      "Epoch 259/300, Loss: 1.068242070052867e-05\n",
      "Epoch 260/300, Loss: 0.00019214582746491558\n",
      "Epoch 261/300, Loss: 0.00017312359111536736\n",
      "Epoch 262/300, Loss: 2.8012655141202758e-05\n",
      "Epoch 263/300, Loss: 6.525314037581235e-05\n",
      "Epoch 264/300, Loss: 0.0001851050729831178\n",
      "Epoch 265/300, Loss: 0.00019077305596750338\n",
      "Epoch 266/300, Loss: 8.258396287661334e-05\n",
      "Epoch 267/300, Loss: 0.00015364479034025478\n",
      "Epoch 268/300, Loss: 0.00010719255173422713\n",
      "Epoch 269/300, Loss: 4.7235737539315446e-05\n",
      "Epoch 270/300, Loss: 3.585841744181476e-05\n",
      "Epoch 271/300, Loss: 2.723090589397184e-05\n",
      "Epoch 272/300, Loss: 1.9781149586456675e-05\n",
      "Epoch 273/300, Loss: 2.713818022402249e-05\n",
      "Epoch 274/300, Loss: 2.9660707313221218e-05\n",
      "Epoch 275/300, Loss: 7.677247873516535e-05\n",
      "Epoch 276/300, Loss: 9.210128838563641e-05\n",
      "Epoch 277/300, Loss: 6.862757698244375e-05\n",
      "Epoch 278/300, Loss: 0.0001292370777928466\n",
      "Epoch 279/300, Loss: 2.780319706636491e-05\n",
      "Epoch 280/300, Loss: 1.9976012239160474e-05\n",
      "Epoch 281/300, Loss: 2.162801884926586e-05\n",
      "Epoch 282/300, Loss: 7.470047530719775e-05\n",
      "Epoch 283/300, Loss: 0.0002503494504313302\n",
      "Epoch 284/300, Loss: 0.00041183857084242366\n",
      "Epoch 285/300, Loss: 0.00020602762474721347\n",
      "Epoch 286/300, Loss: 0.00014262142138633002\n",
      "Epoch 287/300, Loss: 6.908945227367344e-05\n",
      "Epoch 288/300, Loss: 4.025099039345629e-05\n",
      "Epoch 289/300, Loss: 0.0004189940678399676\n",
      "Epoch 290/300, Loss: 0.0014882699878350984\n",
      "Epoch 291/300, Loss: 0.002614125146130008\n",
      "Epoch 292/300, Loss: 0.0006280069149062527\n",
      "Epoch 293/300, Loss: 0.0001983240847254676\n",
      "Epoch 294/300, Loss: 0.0004505015751276101\n",
      "Epoch 295/300, Loss: 0.00026722529092263646\n",
      "Epoch 296/300, Loss: 0.0004058944699935589\n",
      "Epoch 297/300, Loss: 0.0011612841670667923\n",
      "Epoch 298/300, Loss: 0.0013699165255107609\n",
      "Epoch 299/300, Loss: 0.007682750249315225\n",
      "Epoch 300/300, Loss: 0.001779894461641992\n"
     ]
    }
   ],
   "source": [
    "# Training function with more epochs\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=300):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 849.0583904672544, Actual: 841.9999738463275\n",
      "Predicted: 2077.9199664899234, Actual: 2075.0000018859455\n",
      "Predicted: 3552.2758250676793, Actual: 3476.0000095146706\n",
      "Predicted: 485.2833047407962, Actual: 483.9998987803083\n",
      "Predicted: 835.7696902552509, Actual: 831.9999882346683\n",
      "Predicted: 1871.8534262942285, Actual: 1843.9999970174058\n",
      "Predicted: 4376.951784181659, Actual: 4310.000000403015\n",
      "Predicted: 4.960034967076808, Actual: 3.000042845779717\n",
      "Predicted: 3351.228141996005, Actual: 3272.0000079525125\n",
      "Predicted: 2323.7303581087717, Actual: 2313.000012490734\n",
      "Average Loss: 0.0015377839183202014\n"
     ]
    }
   ],
   "source": [
    "# Inverse transform the predictions and target values (if normalized)\n",
    "def inverse_transform(y_scaled):\n",
    "    return y_scaled * y_train_std + y_train_mean\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            total_loss += loss.item()\n",
    "            all_predictions.extend(outputs.squeeze().tolist())\n",
    "            all_labels.extend(labels.tolist())\n",
    "\n",
    "    # Inverse transform the predictions and target values (if normalized)\n",
    "    predictions = inverse_transform(np.array(all_predictions))\n",
    "    actuals = inverse_transform(np.array(all_labels))\n",
    "    \n",
    "    # Print a few predictions and actual values for inspection\n",
    "    for i in range(10):  # Print first 10 for inspection\n",
    "        print(f\"Predicted: {predictions[i]}, Actual: {actuals[i]}\")\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    print(f'Average Loss: {avg_loss}')\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 849.0583904672544, Actual: 841.9999738463275\n",
      "Predicted: 2077.9199664899234, Actual: 2075.0000018859455\n",
      "Predicted: 3552.2758250676793, Actual: 3476.0000095146706\n",
      "Predicted: 485.2833047407962, Actual: 483.9998987803083\n",
      "Predicted: 835.7696902552509, Actual: 831.9999882346683\n",
      "Predicted: 1871.8534262942285, Actual: 1843.9999970174058\n",
      "Predicted: 4376.951784181659, Actual: 4310.000000403015\n",
      "Predicted: 4.960034967076808, Actual: 3.000042845779717\n",
      "Predicted: 3351.228141996005, Actual: 3272.0000079525125\n",
      "Predicted: 2323.7303581087717, Actual: 2313.000012490734\n",
      "Mean Absolute Error: 90.01717423687064\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def evaluate_model_metrics(model, test_loader):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            all_predictions.extend(outputs.squeeze().tolist())\n",
    "            all_labels.extend(labels.tolist())\n",
    "\n",
    "    # Inverse transform the predictions and target values (if normalized)\n",
    "    predictions = inverse_transform(np.array(all_predictions))\n",
    "    actuals = inverse_transform(np.array(all_labels))\n",
    "    \n",
    "    # Calculate Mean Absolute Error\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    \n",
    "    # Print a few predictions and actual values for inspection\n",
    "    for i in range(10):  # Print first 10 for inspection\n",
    "        print(f\"Predicted: {predictions[i]}, Actual: {actuals[i]}\")\n",
    "    \n",
    "    print(f'Mean Absolute Error: {mae}')\n",
    "\n",
    "# Evaluate the model with metrics\n",
    "evaluate_model_metrics(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv_vault import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "model_pt_file = os.getenv('MODEL_PT_FILE')\n",
    "\n",
    "torch.save(model, model_pt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model_with_early_stopping(model, train_loader, criterion, optimizer, num_epochs=100, patience=10):\n",
    "#     model.train()\n",
    "#     min_loss = float('inf')\n",
    "#     epochs_no_improve = 0\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         running_loss = 0.0\n",
    "#         for inputs, labels in train_loader:\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs.squeeze(), labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             running_loss += loss.item()\n",
    "        \n",
    "#         epoch_loss = running_loss / len(train_loader)\n",
    "#         print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}')\n",
    "\n",
    "#         if epoch_loss < min_loss:\n",
    "#             min_loss = epoch_loss\n",
    "#             epochs_no_improve = 0\n",
    "#             best_model = model.state_dict()\n",
    "#         else:\n",
    "#             epochs_no_improve += 1\n",
    "#             if epochs_no_improve == patience:\n",
    "#                 print('Early stopping!')\n",
    "#                 model.load_state_dict(best_model)\n",
    "#                 break\n",
    "\n",
    "# # Train the model with early stopping\n",
    "# train_model_with_early_stopping(model, train_loader, criterion, optimizer)\n",
    "# evaluate_model_metrics(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
