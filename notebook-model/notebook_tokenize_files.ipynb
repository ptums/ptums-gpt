{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/petertumulty/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     filename  \\\n",
      "0               10-25-2023.md   \n",
      "1               02-25-2024.md   \n",
      "2               12-10-2023.md   \n",
      "3  CS-320 Module 6 Content.md   \n",
      "4         Opening Messages.md   \n",
      "\n",
      "                                             content  \n",
      "0  Completing Assignments 1 and 2 was quite chall...  \n",
      "1  Reference: [[@2.Technology]] Lesson Plan: How ...  \n",
      "2  Reference: [[@1.Science]] Good day with Peter!...  \n",
      "3  Created: 10-02-2023 1. https://learn.snhu.edu/...  \n",
      "4  LiveFlow sounds like a fantastic company, and ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "\n",
    "from dotenv_vault import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = os.getenv('PROCESSED_NOTES_CSV')\n",
    "tokenize_notes = os.getenv('TOKENIZE_NOTES')\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             content  \\\n",
      "0  Completing Assignments 1 and 2 was quite chall...   \n",
      "1  Reference: [[@2.Technology]] Lesson Plan: How ...   \n",
      "2  Reference: [[@1.Science]] Good day with Peter!...   \n",
      "3  Created: 10-02-2023 1. https://learn.snhu.edu/...   \n",
      "4  LiveFlow sounds like a fantastic company, and ...   \n",
      "\n",
      "                                     cleaned_content  \n",
      "0  completing assignments 1 and 2 was quite chall...  \n",
      "1  reference 2technology lesson plan how based on...  \n",
      "2  reference 1science good day with peter i was l...  \n",
      "3  created 10022023 1 httpslearnsnhuedud2lleconte...  \n",
      "4  liveflow sounds like a fantastic company and t...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ensure all entries in 'content' are strings and handle missing values\n",
    "df['content'] = df['content'].astype(str).fillna('')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply cleaning and tokenization\n",
    "df['cleaned_content'] = df['content'].apply(clean_text).apply(tokenize_text)\n",
    "\n",
    "# Display the cleaned text\n",
    "print(df[['content', 'cleaned_content']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering complete.\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary\n",
    "all_tokens = [token for tokens in df['cleaned_content'] for token in tokens]\n",
    "vocab = Counter(all_tokens)\n",
    "vocab = {word: i+1 for i, (word, _) in enumerate(vocab.items())}  # +1 to reserve 0 for padding\n",
    "vocab_size = len(vocab) + 1  # +1 for padding token\n",
    "\n",
    "# Parameters\n",
    "embedding_dim = 50  # Dimension of embeddings\n",
    "max_len = max(df['cleaned_content'].apply(len))  # Maximum length of tokenized sequences\n",
    "\n",
    "# Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        text_idx = [vocab.get(word, 0) for word in text]  # Convert words to indices\n",
    "        text_idx = text_idx[:max_len]  # Truncate to max_len\n",
    "        text_idx += [0] * (max_len - len(text_idx))  # Pad sequences\n",
    "        return torch.tensor(text_idx, dtype=torch.long)\n",
    "\n",
    "# Create dataset\n",
    "dataset = TextDataset(df['cleaned_content'].tolist())\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Embedding layer\n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Extract embeddings\n",
    "embeddings = []\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        batch_embeddings = embedding_layer(batch)\n",
    "        batch_embeddings = batch_embeddings.mean(dim=1)  # Average embeddings for each sequence\n",
    "        embeddings.append(batch_embeddings)\n",
    "\n",
    "embeddings = torch.cat(embeddings, dim=0)\n",
    "\n",
    "# Convert embeddings to DataFrame\n",
    "embeddings_df = pd.DataFrame(embeddings.numpy())\n",
    "\n",
    "# Extract metadata: word count and text length\n",
    "df['word_count'] = df['cleaned_content'].apply(lambda x: len(x))\n",
    "df['content_length'] = df['cleaned_content'].apply(lambda x: len(' '.join(x)))\n",
    "\n",
    "# Combine embeddings with metadata\n",
    "final_df = pd.concat([df.drop(columns=['content', 'cleaned_content']), embeddings_df], axis=1)\n",
    "\n",
    "# Save final features to a new CSV file\n",
    "final_df.to_csv(tokenize_notes, index=False)\n",
    "\n",
    "print(\"Feature engineering complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
