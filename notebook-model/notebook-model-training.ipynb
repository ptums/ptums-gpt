{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from dotenv_vault import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = os.getenv('TOKENIZE_NOTES')\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Drop the 'filename' column as it is not needed for training\n",
    "df = df.drop(columns=['filename'])\n",
    "\n",
    "# Split data into features and labels\n",
    "X = df.drop(columns=['word_count', 'content_length'])\n",
    "y = df['word_count']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Normalize the target (if needed)\n",
    "y_train_mean = y_train.mean()\n",
    "y_train_std = y_train.std()\n",
    "y_train_scaled = (y_train - y_train_mean) / y_train_std\n",
    "y_test_scaled = (y_test - y_train_mean) / y_train_std\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_scaled.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_scaled.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotesDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = NotesDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = NotesDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DeeperNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(DeeperNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class EnhancedNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(EnhancedNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, 1)  # For regression, output a single value\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "# Model parameters\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "hidden_dim = 750\n",
    "\n",
    "# Initialize the model\n",
    "# model = DeeperNN(input_dim, hidden_dim)\n",
    "model = EnhancedNN(input_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 0.3617090881851159\n",
      "Epoch 2/300, Loss: 0.023817241320533578\n",
      "Epoch 3/300, Loss: 0.0030673373897259714\n",
      "Epoch 4/300, Loss: 0.0016339382141028948\n",
      "Epoch 5/300, Loss: 0.0003030391701605894\n",
      "Epoch 6/300, Loss: 0.0002523454644879645\n",
      "Epoch 7/300, Loss: 0.0001304606712437817\n",
      "Epoch 8/300, Loss: 0.00016309406074517603\n",
      "Epoch 9/300, Loss: 0.0001270425018925667\n",
      "Epoch 10/300, Loss: 0.00010509803096762258\n",
      "Epoch 11/300, Loss: 9.527269383780597e-05\n",
      "Epoch 12/300, Loss: 0.00011750283465516986\n",
      "Epoch 13/300, Loss: 6.009580822233387e-05\n",
      "Epoch 14/300, Loss: 8.376509310293936e-05\n",
      "Epoch 15/300, Loss: 0.00012020920856134341\n",
      "Epoch 16/300, Loss: 0.00013321374985695767\n",
      "Epoch 17/300, Loss: 0.00019471174212077656\n",
      "Epoch 18/300, Loss: 0.0005775285841848113\n",
      "Epoch 19/300, Loss: 0.0001842731895450063\n",
      "Epoch 20/300, Loss: 0.0001271913396713468\n",
      "Epoch 21/300, Loss: 0.00012173298991432315\n",
      "Epoch 22/300, Loss: 6.882045728448965e-05\n",
      "Epoch 23/300, Loss: 0.00011751337749381874\n",
      "Epoch 24/300, Loss: 0.0001947988986390364\n",
      "Epoch 25/300, Loss: 0.00015262047571396628\n",
      "Epoch 26/300, Loss: 0.00034042077224608205\n",
      "Epoch 27/300, Loss: 0.000919402258516225\n",
      "Epoch 28/300, Loss: 0.0007726824113840274\n",
      "Epoch 29/300, Loss: 0.0003613458659042647\n",
      "Epoch 30/300, Loss: 0.0006282682290810427\n",
      "Epoch 31/300, Loss: 0.0009328512848792583\n",
      "Epoch 32/300, Loss: 0.00014968655712341148\n",
      "Epoch 33/300, Loss: 0.00017095465409511235\n",
      "Epoch 34/300, Loss: 9.26658952410339e-05\n",
      "Epoch 35/300, Loss: 7.487690953966146e-05\n",
      "Epoch 36/300, Loss: 5.6953722731266586e-05\n",
      "Epoch 37/300, Loss: 0.0001426764262616512\n",
      "Epoch 38/300, Loss: 0.0002054403246706658\n",
      "Epoch 39/300, Loss: 0.000301737868611733\n",
      "Epoch 40/300, Loss: 0.00021684292144491816\n",
      "Epoch 41/300, Loss: 0.00014938655241723465\n",
      "Epoch 42/300, Loss: 0.00016643742361267008\n",
      "Epoch 43/300, Loss: 0.00031737169217051003\n",
      "Epoch 44/300, Loss: 8.301254389439954e-05\n",
      "Epoch 45/300, Loss: 0.00021171139107871952\n",
      "Epoch 46/300, Loss: 0.00015289291796331158\n",
      "Epoch 47/300, Loss: 0.00018726643841211242\n",
      "Epoch 48/300, Loss: 0.00010275108797941357\n",
      "Epoch 49/300, Loss: 8.661085768692014e-05\n",
      "Epoch 50/300, Loss: 0.0001658458645698901\n",
      "Epoch 51/300, Loss: 0.0002515726031752389\n",
      "Epoch 52/300, Loss: 0.001070867227673788\n",
      "Epoch 53/300, Loss: 0.001191747544788469\n",
      "Epoch 54/300, Loss: 0.0009638841880156466\n",
      "Epoch 55/300, Loss: 0.007471664580742454\n",
      "Epoch 56/300, Loss: 0.035857822217574144\n",
      "Epoch 57/300, Loss: 0.030731858084580285\n",
      "Epoch 58/300, Loss: 0.004735380663796126\n",
      "Epoch 59/300, Loss: 0.0024620224886994515\n",
      "Epoch 60/300, Loss: 0.0013997745367496943\n",
      "Epoch 61/300, Loss: 0.00028500037995782854\n",
      "Epoch 62/300, Loss: 0.00016820467937194686\n",
      "Epoch 63/300, Loss: 0.00018139561172191812\n",
      "Epoch 64/300, Loss: 0.00014933862793872036\n",
      "Epoch 65/300, Loss: 0.0001569972568039415\n",
      "Epoch 66/300, Loss: 6.931453245478654e-05\n",
      "Epoch 67/300, Loss: 4.762187999356109e-05\n",
      "Epoch 68/300, Loss: 4.9232251189766286e-05\n",
      "Epoch 69/300, Loss: 4.3875394734832775e-05\n",
      "Epoch 70/300, Loss: 3.4749231633070034e-05\n",
      "Epoch 71/300, Loss: 4.11551993094786e-05\n",
      "Epoch 72/300, Loss: 3.8458757288888014e-05\n",
      "Epoch 73/300, Loss: 4.531156358959714e-05\n",
      "Epoch 74/300, Loss: 6.35952011168085e-05\n",
      "Epoch 75/300, Loss: 0.0008507529444398786\n",
      "Epoch 76/300, Loss: 0.0035874639633172926\n",
      "Epoch 77/300, Loss: 0.027311804884735273\n",
      "Epoch 78/300, Loss: 0.016448215005527202\n",
      "Epoch 79/300, Loss: 0.0019002508453779324\n",
      "Epoch 80/300, Loss: 0.0005834867985724582\n",
      "Epoch 81/300, Loss: 0.0007400831775014108\n",
      "Epoch 82/300, Loss: 0.00031873084828092893\n",
      "Epoch 83/300, Loss: 0.00021358773429519236\n",
      "Epoch 84/300, Loss: 0.00022400257377117478\n",
      "Epoch 85/300, Loss: 4.0867699537652936e-05\n",
      "Epoch 86/300, Loss: 4.1910095738354646e-05\n",
      "Epoch 87/300, Loss: 6.498034845866354e-05\n",
      "Epoch 88/300, Loss: 3.7904334273039954e-05\n",
      "Epoch 89/300, Loss: 0.00011418589716820287\n",
      "Epoch 90/300, Loss: 4.9570210218294766e-05\n",
      "Epoch 91/300, Loss: 7.68922341433752e-05\n",
      "Epoch 92/300, Loss: 4.380179033384928e-05\n",
      "Epoch 93/300, Loss: 7.407769811307892e-05\n",
      "Epoch 94/300, Loss: 4.311781010289307e-05\n",
      "Epoch 95/300, Loss: 3.342937582386328e-05\n",
      "Epoch 96/300, Loss: 3.661283443833734e-05\n",
      "Epoch 97/300, Loss: 2.997836006551025e-05\n",
      "Epoch 98/300, Loss: 2.3184566116110784e-05\n",
      "Epoch 99/300, Loss: 3.384294195173329e-05\n",
      "Epoch 100/300, Loss: 3.0549219301322316e-05\n",
      "Epoch 101/300, Loss: 3.8586065987769156e-05\n",
      "Epoch 102/300, Loss: 2.6873413590142025e-05\n",
      "Epoch 103/300, Loss: 1.9185653208727383e-05\n",
      "Epoch 104/300, Loss: 2.260236587627979e-05\n",
      "Epoch 105/300, Loss: 2.746605083889096e-05\n",
      "Epoch 106/300, Loss: 1.884039369614536e-05\n",
      "Epoch 107/300, Loss: 1.9168485622637826e-05\n",
      "Epoch 108/300, Loss: 2.052047462759794e-05\n",
      "Epoch 109/300, Loss: 2.845861001339089e-05\n",
      "Epoch 110/300, Loss: 5.237878925575052e-05\n",
      "Epoch 111/300, Loss: 3.667480107675434e-05\n",
      "Epoch 112/300, Loss: 1.809490223978939e-05\n",
      "Epoch 113/300, Loss: 1.7290195824228393e-05\n",
      "Epoch 114/300, Loss: 2.2201696256879106e-05\n",
      "Epoch 115/300, Loss: 2.2086975401467307e-05\n",
      "Epoch 116/300, Loss: 1.4523047106625822e-05\n",
      "Epoch 117/300, Loss: 1.5597993075588373e-05\n",
      "Epoch 118/300, Loss: 3.9096106637484094e-05\n",
      "Epoch 119/300, Loss: 9.540748063548228e-05\n",
      "Epoch 120/300, Loss: 0.00020369453493099412\n",
      "Epoch 121/300, Loss: 0.0002055315989112291\n",
      "Epoch 122/300, Loss: 0.00013280036535577797\n",
      "Epoch 123/300, Loss: 0.0001312660650802699\n",
      "Epoch 124/300, Loss: 8.31539343595834e-05\n",
      "Epoch 125/300, Loss: 4.215100357453737e-05\n",
      "Epoch 126/300, Loss: 2.8485918368173052e-05\n",
      "Epoch 127/300, Loss: 2.181110989185024e-05\n",
      "Epoch 128/300, Loss: 1.9564726672246923e-05\n",
      "Epoch 129/300, Loss: 1.738088651922896e-05\n",
      "Epoch 130/300, Loss: 4.416423687297012e-05\n",
      "Epoch 131/300, Loss: 2.971002793042556e-05\n",
      "Epoch 132/300, Loss: 2.7092654686624118e-05\n",
      "Epoch 133/300, Loss: 2.0057568370272127e-05\n",
      "Epoch 134/300, Loss: 2.362656757788117e-05\n",
      "Epoch 135/300, Loss: 4.124406318265086e-05\n",
      "Epoch 136/300, Loss: 4.147100470151456e-05\n",
      "Epoch 137/300, Loss: 7.338997857543386e-05\n",
      "Epoch 138/300, Loss: 0.00010345198165850888\n",
      "Epoch 139/300, Loss: 5.427310562529448e-05\n",
      "Epoch 140/300, Loss: 0.00032722339981498745\n",
      "Epoch 141/300, Loss: 0.0006825028108981936\n",
      "Epoch 142/300, Loss: 0.0001307762155839124\n",
      "Epoch 143/300, Loss: 0.0011392613211539452\n",
      "Epoch 144/300, Loss: 0.00036504479305078446\n",
      "Epoch 145/300, Loss: 0.0002308645656332948\n",
      "Epoch 146/300, Loss: 0.00019621316137449627\n",
      "Epoch 147/300, Loss: 0.0006512244824803977\n",
      "Epoch 148/300, Loss: 0.0011007616096417462\n",
      "Epoch 149/300, Loss: 0.0010554982967445731\n",
      "Epoch 150/300, Loss: 0.0004425156814345667\n",
      "Epoch 151/300, Loss: 0.0002931054744931915\n",
      "Epoch 152/300, Loss: 0.0001003309965342755\n",
      "Epoch 153/300, Loss: 0.00010166923280657532\n",
      "Epoch 154/300, Loss: 3.552304964446912e-05\n",
      "Epoch 155/300, Loss: 5.2276480899811426e-05\n",
      "Epoch 156/300, Loss: 2.4255445521846826e-05\n",
      "Epoch 157/300, Loss: 3.3608349701359274e-05\n",
      "Epoch 158/300, Loss: 9.175261364116762e-05\n",
      "Epoch 159/300, Loss: 0.0004917787100088102\n",
      "Epoch 160/300, Loss: 0.00017435618352871978\n",
      "Epoch 161/300, Loss: 5.525446988784959e-05\n",
      "Epoch 162/300, Loss: 3.799502407051286e-05\n",
      "Epoch 163/300, Loss: 7.260406311621936e-05\n",
      "Epoch 164/300, Loss: 5.460292225890091e-05\n",
      "Epoch 165/300, Loss: 4.882502557130034e-05\n",
      "Epoch 166/300, Loss: 0.0002237315615765941\n",
      "Epoch 167/300, Loss: 0.00018472016367803392\n",
      "Epoch 168/300, Loss: 0.0005770749015171436\n",
      "Epoch 169/300, Loss: 0.0018177838817696045\n",
      "Epoch 170/300, Loss: 0.00567738303748468\n",
      "Epoch 171/300, Loss: 0.0004797427442386642\n",
      "Epoch 172/300, Loss: 0.009136520520673673\n",
      "Epoch 173/300, Loss: 0.004359210268854727\n",
      "Epoch 174/300, Loss: 0.0010864810042939528\n",
      "Epoch 175/300, Loss: 0.0002795782718055163\n",
      "Epoch 176/300, Loss: 0.00013970473186849345\n",
      "Epoch 177/300, Loss: 4.073384740135416e-05\n",
      "Epoch 178/300, Loss: 3.9563767191602416e-05\n",
      "Epoch 179/300, Loss: 3.228594099662012e-05\n",
      "Epoch 180/300, Loss: 2.382004363485784e-05\n",
      "Epoch 181/300, Loss: 4.3648631348284985e-05\n",
      "Epoch 182/300, Loss: 5.918946312015961e-05\n",
      "Epoch 183/300, Loss: 3.045860474434161e-05\n",
      "Epoch 184/300, Loss: 2.548786562783789e-05\n",
      "Epoch 185/300, Loss: 3.568447900866096e-05\n",
      "Epoch 186/300, Loss: 3.097672879149334e-05\n",
      "Epoch 187/300, Loss: 3.951640992514321e-05\n",
      "Epoch 188/300, Loss: 4.024946279837662e-05\n",
      "Epoch 189/300, Loss: 0.00011104338086385581\n",
      "Epoch 190/300, Loss: 0.00026872099957281055\n",
      "Epoch 191/300, Loss: 0.000499770317075768\n",
      "Epoch 192/300, Loss: 0.0016203770388457134\n",
      "Epoch 193/300, Loss: 0.0010404032956749258\n",
      "Epoch 194/300, Loss: 0.0011165433833842766\n",
      "Epoch 195/300, Loss: 0.00029470323058352444\n",
      "Epoch 196/300, Loss: 7.85886992808197e-05\n",
      "Epoch 197/300, Loss: 0.00011278723775675596\n",
      "Epoch 198/300, Loss: 0.0003207070049747448\n",
      "Epoch 199/300, Loss: 0.0004961287158978752\n",
      "Epoch 200/300, Loss: 0.0007430027473770009\n",
      "Epoch 201/300, Loss: 0.00102293801338467\n",
      "Epoch 202/300, Loss: 0.006035565733423266\n",
      "Epoch 203/300, Loss: 0.0016516067099282257\n",
      "Epoch 204/300, Loss: 0.0054798572019104826\n",
      "Epoch 205/300, Loss: 0.011587366451414326\n",
      "Epoch 206/300, Loss: 0.049825451902793964\n",
      "Epoch 207/300, Loss: 0.024194896765708875\n",
      "Epoch 208/300, Loss: 0.0014437013085051532\n",
      "Epoch 209/300, Loss: 0.001293326210854598\n",
      "Epoch 210/300, Loss: 0.0018010806106997877\n",
      "Epoch 211/300, Loss: 0.0008700276926471193\n",
      "Epoch 212/300, Loss: 0.0004420817593403339\n",
      "Epoch 213/300, Loss: 0.0031163518031247037\n",
      "Epoch 214/300, Loss: 0.0011664985332141729\n",
      "Epoch 215/300, Loss: 0.0015066250669968596\n",
      "Epoch 216/300, Loss: 0.0023797207798835237\n",
      "Epoch 217/300, Loss: 0.00014178517619711783\n",
      "Epoch 218/300, Loss: 0.00022855582868655083\n",
      "Epoch 219/300, Loss: 0.00012721616404202281\n",
      "Epoch 220/300, Loss: 6.565567909882175e-05\n",
      "Epoch 221/300, Loss: 0.00013555219244440914\n",
      "Epoch 222/300, Loss: 0.0002877177945896691\n",
      "Epoch 223/300, Loss: 9.294106303657732e-05\n",
      "Epoch 224/300, Loss: 5.9092634949975996e-05\n",
      "Epoch 225/300, Loss: 4.465475255624609e-05\n",
      "Epoch 226/300, Loss: 2.731341595712796e-05\n",
      "Epoch 227/300, Loss: 6.0547004370379765e-05\n",
      "Epoch 228/300, Loss: 0.00012916947860886516\n",
      "Epoch 229/300, Loss: 9.018152475523254e-05\n",
      "Epoch 230/300, Loss: 4.237530398959758e-05\n",
      "Epoch 231/300, Loss: 3.969803781513436e-05\n",
      "Epoch 232/300, Loss: 2.375353774278952e-05\n",
      "Epoch 233/300, Loss: 1.9700888651342258e-05\n",
      "Epoch 234/300, Loss: 2.928849775535606e-05\n",
      "Epoch 235/300, Loss: 3.621318546720431e-05\n",
      "Epoch 236/300, Loss: 2.686020740994584e-05\n",
      "Epoch 237/300, Loss: 2.1436774055042695e-05\n",
      "Epoch 238/300, Loss: 3.9693068111701955e-05\n",
      "Epoch 239/300, Loss: 3.139880439262972e-05\n",
      "Epoch 240/300, Loss: 2.57270465699309e-05\n",
      "Epoch 241/300, Loss: 2.0538695510271953e-05\n",
      "Epoch 242/300, Loss: 3.373704734749002e-05\n",
      "Epoch 243/300, Loss: 3.028539421584834e-05\n",
      "Epoch 244/300, Loss: 2.4007959498966986e-05\n",
      "Epoch 245/300, Loss: 2.4558366440511524e-05\n",
      "Epoch 246/300, Loss: 2.065307621466529e-05\n",
      "Epoch 247/300, Loss: 4.982954170061834e-05\n",
      "Epoch 248/300, Loss: 5.744082688794917e-05\n",
      "Epoch 249/300, Loss: 0.00012241768558103678\n",
      "Epoch 250/300, Loss: 3.36033608084114e-05\n",
      "Epoch 251/300, Loss: 7.310956190945275e-05\n",
      "Epoch 252/300, Loss: 5.9039738768038465e-05\n",
      "Epoch 253/300, Loss: 6.99201929885356e-05\n",
      "Epoch 254/300, Loss: 3.0008839590472664e-05\n",
      "Epoch 255/300, Loss: 1.838360370991148e-05\n",
      "Epoch 256/300, Loss: 1.362775340471432e-05\n",
      "Epoch 257/300, Loss: 1.7162073329532895e-05\n",
      "Epoch 258/300, Loss: 2.065842865156834e-05\n",
      "Epoch 259/300, Loss: 3.4522655185080444e-05\n",
      "Epoch 260/300, Loss: 0.00036415953767076595\n",
      "Epoch 261/300, Loss: 0.0004027125539180002\n",
      "Epoch 262/300, Loss: 0.000490678495464779\n",
      "Epoch 263/300, Loss: 0.00013219148695498006\n",
      "Epoch 264/300, Loss: 5.7061127437775306e-05\n",
      "Epoch 265/300, Loss: 3.4829476195036e-05\n",
      "Epoch 266/300, Loss: 5.891982962829819e-05\n",
      "Epoch 267/300, Loss: 2.5679167139826047e-05\n",
      "Epoch 268/300, Loss: 3.705040443504147e-05\n",
      "Epoch 269/300, Loss: 3.7393983374936635e-05\n",
      "Epoch 270/300, Loss: 7.755782965695719e-05\n",
      "Epoch 271/300, Loss: 0.0003413162072846669\n",
      "Epoch 272/300, Loss: 0.0005576682962998204\n",
      "Epoch 273/300, Loss: 0.00017915039030700237\n",
      "Epoch 274/300, Loss: 3.5396003436529244e-05\n",
      "Epoch 275/300, Loss: 4.102871472050059e-05\n",
      "Epoch 276/300, Loss: 6.105523125131042e-05\n",
      "Epoch 277/300, Loss: 2.5989566739183017e-05\n",
      "Epoch 278/300, Loss: 1.705106073831973e-05\n",
      "Epoch 279/300, Loss: 1.6006467920928308e-05\n",
      "Epoch 280/300, Loss: 1.378532793170818e-05\n",
      "Epoch 281/300, Loss: 1.7327632585755437e-05\n",
      "Epoch 282/300, Loss: 1.2173813191015812e-05\n",
      "Epoch 283/300, Loss: 1.0829249928322401e-05\n",
      "Epoch 284/300, Loss: 1.1808856036647117e-05\n",
      "Epoch 285/300, Loss: 1.0625341387463108e-05\n",
      "Epoch 286/300, Loss: 2.752064274527823e-05\n",
      "Epoch 287/300, Loss: 1.674468856624332e-05\n",
      "Epoch 288/300, Loss: 1.7850698243723617e-05\n",
      "Epoch 289/300, Loss: 4.5457486707116336e-05\n",
      "Epoch 290/300, Loss: 1.7540521374832296e-05\n",
      "Epoch 291/300, Loss: 1.1281774108415208e-05\n",
      "Epoch 292/300, Loss: 1.2246874617678833e-05\n",
      "Epoch 293/300, Loss: 3.863568885938213e-05\n",
      "Epoch 294/300, Loss: 2.135853922206814e-05\n",
      "Epoch 295/300, Loss: 1.9201633350755737e-05\n",
      "Epoch 296/300, Loss: 1.411132433079992e-05\n",
      "Epoch 297/300, Loss: 1.0704192795018522e-05\n",
      "Epoch 298/300, Loss: 1.1577481380323948e-05\n",
      "Epoch 299/300, Loss: 9.038642372759473e-06\n",
      "Epoch 300/300, Loss: 8.021208064914581e-06\n"
     ]
    }
   ],
   "source": [
    "# Training function with more epochs\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=300):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 850.7671393786809, Actual: 841.9999738463275\n",
      "Predicted: 2074.46974592518, Actual: 2075.0000018859455\n",
      "Predicted: 3471.304006238569, Actual: 3476.0000095146706\n",
      "Predicted: 495.8350980063492, Actual: 483.9998987803083\n",
      "Predicted: 837.8500346188603, Actual: 831.9999882346683\n",
      "Predicted: 1859.6767723702092, Actual: 1843.9999970174058\n",
      "Predicted: 4283.138370559117, Actual: 4310.000000403015\n",
      "Predicted: 18.941340202263746, Actual: 3.000042845779717\n",
      "Predicted: 3280.4469543182586, Actual: 3272.0000079525125\n",
      "Predicted: 2314.2545423440474, Actual: 2313.000012490734\n",
      "Average Loss: 5.033277611801168e-05\n"
     ]
    }
   ],
   "source": [
    "# Inverse transform the predictions and target values (if normalized)\n",
    "def inverse_transform(y_scaled):\n",
    "    return y_scaled * y_train_std + y_train_mean\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            total_loss += loss.item()\n",
    "            all_predictions.extend(outputs.squeeze().tolist())\n",
    "            all_labels.extend(labels.tolist())\n",
    "\n",
    "    # Inverse transform the predictions and target values (if normalized)\n",
    "    predictions = inverse_transform(np.array(all_predictions))\n",
    "    actuals = inverse_transform(np.array(all_labels))\n",
    "    \n",
    "    # Print a few predictions and actual values for inspection\n",
    "    for i in range(10):  # Print first 10 for inspection\n",
    "        print(f\"Predicted: {predictions[i]}, Actual: {actuals[i]}\")\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    print(f'Average Loss: {avg_loss}')\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 850.7671393786809, Actual: 841.9999738463275\n",
      "Predicted: 2074.46974592518, Actual: 2075.0000018859455\n",
      "Predicted: 3471.304006238569, Actual: 3476.0000095146706\n",
      "Predicted: 495.8350980063492, Actual: 483.9998987803083\n",
      "Predicted: 837.8500346188603, Actual: 831.9999882346683\n",
      "Predicted: 1859.6767723702092, Actual: 1843.9999970174058\n",
      "Predicted: 4283.138370559117, Actual: 4310.000000403015\n",
      "Predicted: 18.941340202263746, Actual: 3.000042845779717\n",
      "Predicted: 3280.4469543182586, Actual: 3272.0000079525125\n",
      "Predicted: 2314.2545423440474, Actual: 2313.000012490734\n",
      "Mean Absolute Error: 22.3380710387503\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def evaluate_model_metrics(model, test_loader):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            all_predictions.extend(outputs.squeeze().tolist())\n",
    "            all_labels.extend(labels.tolist())\n",
    "\n",
    "    # Inverse transform the predictions and target values (if normalized)\n",
    "    predictions = inverse_transform(np.array(all_predictions))\n",
    "    actuals = inverse_transform(np.array(all_labels))\n",
    "    \n",
    "    # Calculate Mean Absolute Error\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    \n",
    "    # Print a few predictions and actual values for inspection\n",
    "    for i in range(10):  # Print first 10 for inspection\n",
    "        print(f\"Predicted: {predictions[i]}, Actual: {actuals[i]}\")\n",
    "    \n",
    "    print(f'Mean Absolute Error: {mae}')\n",
    "\n",
    "# Evaluate the model with metrics\n",
    "evaluate_model_metrics(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 1.070787970404222e-05\n",
      "Epoch 2/100, Loss: 1.3304890591083868e-05\n",
      "Epoch 3/100, Loss: 1.7515641375017533e-05\n",
      "Epoch 4/100, Loss: 0.00019177833277545257\n",
      "Epoch 5/100, Loss: 0.00012902311679777325\n",
      "Epoch 6/100, Loss: 3.5472243636927726e-05\n",
      "Epoch 7/100, Loss: 2.588022356289194e-05\n",
      "Epoch 8/100, Loss: 0.00011899734102515045\n",
      "Epoch 9/100, Loss: 0.00020484695418369818\n",
      "Epoch 10/100, Loss: 0.0004991115466198512\n",
      "Epoch 11/100, Loss: 0.002723925696763648\n",
      "Early stopping!\n",
      "Predicted: 841.805218203777, Actual: 841.9999738463275\n",
      "Predicted: 2082.1672308450397, Actual: 2075.0000018859455\n",
      "Predicted: 3433.1883878659605, Actual: 3476.0000095146706\n",
      "Predicted: 482.3642886220905, Actual: 483.9998987803083\n",
      "Predicted: 829.6407892010432, Actual: 831.9999882346683\n",
      "Predicted: 1865.8287005390798, Actual: 1843.9999970174058\n",
      "Predicted: 4216.1145018997895, Actual: 4310.000000403015\n",
      "Predicted: 3.102057706163123, Actual: 3.000042845779717\n",
      "Predicted: 3228.680841295943, Actual: 3272.0000079525125\n",
      "Predicted: 2323.8487870337262, Actual: 2313.000012490734\n",
      "Mean Absolute Error: 64.15552506402092\n"
     ]
    }
   ],
   "source": [
    "# def train_model_with_early_stopping(model, train_loader, criterion, optimizer, num_epochs=100, patience=10):\n",
    "#     model.train()\n",
    "#     min_loss = float('inf')\n",
    "#     epochs_no_improve = 0\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         running_loss = 0.0\n",
    "#         for inputs, labels in train_loader:\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs.squeeze(), labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             running_loss += loss.item()\n",
    "        \n",
    "#         epoch_loss = running_loss / len(train_loader)\n",
    "#         print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}')\n",
    "\n",
    "#         if epoch_loss < min_loss:\n",
    "#             min_loss = epoch_loss\n",
    "#             epochs_no_improve = 0\n",
    "#             best_model = model.state_dict()\n",
    "#         else:\n",
    "#             epochs_no_improve += 1\n",
    "#             if epochs_no_improve == patience:\n",
    "#                 print('Early stopping!')\n",
    "#                 model.load_state_dict(best_model)\n",
    "#                 break\n",
    "\n",
    "# # Train the model with early stopping\n",
    "# train_model_with_early_stopping(model, train_loader, criterion, optimizer)\n",
    "# evaluate_model_metrics(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
